{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "image_path ='./'\n",
    "train_dataset = datasets.CelebA(\n",
    "    image_path, split='train',\n",
    "    target_type='attr', download=True\n",
    ")\n",
    "valid_dataset = datasets.CelebA(\n",
    "    image_path, split='valid',\n",
    "    target_type='attr', download=True\n",
    ")\n",
    "test_dataset = datasets.CelebA(\n",
    "    image_path, split='test',\n",
    "    target_type='attr', download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_smile = lambda attr: attr[18]\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop([178, 178]),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize([64, 64]),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop([178, 178]),\n",
    "    transforms.Resize([64, 64]),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.CelebA(\n",
    "    image_path, split='train',\n",
    "    target_type='attr', download=False,\n",
    "    transform=transform_train, target_transform=get_smile\n",
    ")\n",
    "valid_dataset = datasets.CelebA(\n",
    "    image_path, split='valid',\n",
    "    target_type='attr', download=False,\n",
    "    transform=transform, target_transform=get_smile\n",
    ")\n",
    "test_dataset = datasets.CelebA(\n",
    "    image_path, split='test',\n",
    "    target_type='attr', download=False,\n",
    "    transform=transform, target_transform=get_smile\n",
    ")\n",
    "train_dataset = Subset(\n",
    "    train_dataset, torch.arange(16_000)\n",
    ")\n",
    "valid_dataset = Subset(\n",
    "    valid_dataset, torch.arange(1_000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmileClassificationNet(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=3, out_channels=32,\n",
    "                kernel_size=3, padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Conv2d(\n",
    "                in_channels=32, out_channels=64,\n",
    "                kernel_size=3, padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Conv2d(\n",
    "                in_channels=64, out_channels=128,\n",
    "                kernel_size=3, padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(\n",
    "                in_channels=128, out_channels=256,\n",
    "                kernel_size=3, padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=8),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "    \n",
    "model = SmileClassificationNet(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, n_epochs, train_loader, valid_loader):\n",
    "    loss_history_train = [0] * n_epochs\n",
    "    acc_history_train = [0] * n_epochs\n",
    "    loss_history_valid = [0] * n_epochs\n",
    "    acc_history_valid = [0] * n_epochs\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            prediction = model(x_batch)[:, 0]\n",
    "            loss_val = loss(prediction, y_batch.float())\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_history_train[epoch] += loss_val.item() * y_batch.size(0)\n",
    "            is_correct = ((prediction >= 0.5).float() == y_batch).float()\n",
    "            acc_history_train[epoch] += is_correct.sum()\n",
    "        loss_history_train[epoch] /= len(train_loader.dataset)\n",
    "        acc_history_train[epoch] /= len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in valid_loader:\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                \n",
    "                prediction = model(x_batch)[:, 0]\n",
    "                loss_val = loss(prediction, y_batch.float())\n",
    "                loss_history_valid[epoch] += loss_val.item() * y_batch.size(0)\n",
    "                is_correct = ((prediction >= 0.5).float() == y_batch).float()\n",
    "                acc_history_valid[epoch] += is_correct.sum()\n",
    "        loss_history_valid[epoch] /= len(valid_loader.dataset)\n",
    "        acc_history_valid[epoch] /= len(valid_loader.dataset)\n",
    "\n",
    "        print(f'Epoch {epoch+1} accuracy:'\n",
    "              f'{acc_history_train[epoch]:.4f} val_accuracy: '\n",
    "              f'{acc_history_valid[epoch]:.4f}')\n",
    "    return loss_history_train, loss_history_valid,\\\n",
    "           acc_history_train, acc_history_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 accuracy:0.6163 val_accuracy: 0.6430\n",
      "Epoch 2 accuracy:0.6880 val_accuracy: 0.7290\n",
      "Epoch 3 accuracy:0.7261 val_accuracy: 0.7550\n",
      "Epoch 4 accuracy:0.7380 val_accuracy: 0.7730\n",
      "Epoch 5 accuracy:0.7511 val_accuracy: 0.7660\n",
      "Epoch 6 accuracy:0.7614 val_accuracy: 0.7710\n",
      "Epoch 7 accuracy:0.7730 val_accuracy: 0.7950\n",
      "Epoch 8 accuracy:0.7931 val_accuracy: 0.8110\n",
      "Epoch 9 accuracy:0.8096 val_accuracy: 0.8190\n",
      "Epoch 10 accuracy:0.8250 val_accuracy: 0.8330\n",
      "Epoch 11 accuracy:0.8336 val_accuracy: 0.8520\n",
      "Epoch 12 accuracy:0.8384 val_accuracy: 0.8630\n",
      "Epoch 13 accuracy:0.8371 val_accuracy: 0.8620\n",
      "Epoch 14 accuracy:0.8512 val_accuracy: 0.8560\n",
      "Epoch 15 accuracy:0.8479 val_accuracy: 0.8730\n",
      "Epoch 16 accuracy:0.8572 val_accuracy: 0.8740\n",
      "Epoch 17 accuracy:0.8588 val_accuracy: 0.8680\n",
      "Epoch 18 accuracy:0.8654 val_accuracy: 0.8720\n",
      "Epoch 19 accuracy:0.8631 val_accuracy: 0.8690\n",
      "Epoch 20 accuracy:0.8606 val_accuracy: 0.8820\n",
      "Epoch 21 accuracy:0.8679 val_accuracy: 0.8880\n",
      "Epoch 22 accuracy:0.8664 val_accuracy: 0.8840\n",
      "Epoch 23 accuracy:0.8728 val_accuracy: 0.8910\n",
      "Epoch 24 accuracy:0.8697 val_accuracy: 0.8840\n",
      "Epoch 25 accuracy:0.8729 val_accuracy: 0.8770\n",
      "Epoch 26 accuracy:0.8731 val_accuracy: 0.8820\n",
      "Epoch 27 accuracy:0.8700 val_accuracy: 0.8790\n",
      "Epoch 28 accuracy:0.8792 val_accuracy: 0.8820\n",
      "Epoch 29 accuracy:0.8768 val_accuracy: 0.8790\n",
      "Epoch 30 accuracy:0.8799 val_accuracy: 0.8750\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 30\n",
    "\n",
    "hist = train(model, n_epochs, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
